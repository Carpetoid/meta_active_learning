{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "from __future__ import print_function, division\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n",
    "    cls_num = 10\n",
    "\n",
    "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=True):\n",
    "        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
    "        np.random.seed(rand_number)\n",
    "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
    "        self.gen_imbalanced_data(img_num_list)\n",
    "\n",
    "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
    "        img_max = len(self.data) / cls_num\n",
    "        img_num_per_cls = []\n",
    "        if imb_type == 'exp':\n",
    "            for cls_idx in range(cls_num):\n",
    "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "                img_num_per_cls.append(int(num))\n",
    "        elif imb_type == 'step':\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max))\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max * imb_factor))\n",
    "        else:\n",
    "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "        return img_num_per_cls\n",
    "\n",
    "    def gen_imbalanced_data(self, img_num_per_cls):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        targets_np = np.array(self.targets, dtype=np.int64)\n",
    "        classes = np.unique(targets_np)\n",
    "        # np.random.shuffle(classes)\n",
    "        self.num_per_cls_dict = dict()\n",
    "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "            self.num_per_cls_dict[the_class] = the_img_num\n",
    "            idx = np.where(targets_np == the_class)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            selec_idx = idx[:the_img_num]\n",
    "            new_data.append(self.data[selec_idx, ...])\n",
    "            new_targets.extend([the_class, ] * the_img_num)\n",
    "        new_data = np.vstack(new_data)\n",
    "        self.data = new_data\n",
    "        self.targets = new_targets\n",
    "        \n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.cls_num):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "class IMBALANCECIFAR100(IMBALANCECIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }\n",
    "    cls_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtxYeHjRVO9S"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for testing\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # TODO 1, data transform for train set. Use RandomHorizontalFlip function for data augumentation\n",
    "        ################################ TODO ##################################\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "        ################################ TODO ##################################\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 (imbalanced)\n",
    "\n",
    "image_datasets= {x: IMBALANCECIFAR10(root='./data', train=(x=='train'), download=True, transform=data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=(x=='train'), num_workers=16) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "#print(dataset_sizes)\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, (3,3), stride=(1,1),padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, (3,3), stride=(1,1), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False),            \n",
    "            \n",
    "            nn.Conv2d(64, 128, (3,3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(128, 128, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(128),            \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),            \n",
    "            \n",
    "            nn.Conv2d(128, 256, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(256),            \n",
    "            nn.ReLU(),         \n",
    "            \n",
    "            nn.Conv2d(256, 256, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(256),            \n",
    "            nn.ReLU(),  \n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(512),            \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 512, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(512),            \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(512, 512, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(512),            \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 512, (3,3),padding=1),\n",
    "            nn.BatchNorm2d(512),            \n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512,128),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(128,10)\n",
    "        )    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, 512)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model_mynet_ft = MyNet()\n",
    "model_mynet_ft = model_mynet_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_Y2cgPEVO9X"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs, dataloader, size=None, save_path='saved_weight.pth'):\n",
    "    since = time.time()\n",
    "    loss_record = [] # Frost: for plot\n",
    "    dataset_sizes_train = size\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train': model.train()  # Set model to training mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                #import pdb; pdb.set_trace()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes_train\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes_train\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            loss_record.append(epoch_loss)\n",
    "            \n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        model_test = model\n",
    "        test_model(model_test, save_path)\n",
    "    print()\n",
    "\n",
    "    plt.plot(loss_record)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    model_test = model\n",
    "    test_model(model, save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtE6iv9rVO9w"
   },
   "outputs": [],
   "source": [
    "def test_model(model, load_path='saved_weight.pth'):    \n",
    "    # load the model weights\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    for phase in ['test']:\n",
    "        if phase == 'test':\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ################################ TODO ##################################\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.max(outputs, 1)[1]\n",
    "                ################################ TODO ##################################\n",
    "\n",
    "            # statistics\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print('{} Acc: {:.4f}'.format(phase, epoch_acc))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_most_inform():\n",
    "    def __init__(self, image_tensor, labels, transform=data_transforms, target_transform=None):\n",
    "        self.img_labels = labels #list\n",
    "        self.img_tensors = image_tensor #also list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img_tensors[idx]\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "    \n",
    "def sampling(model, dataset, load_path='saved_weight.pth', ):\n",
    "    model.load_state_dict(torch.load(load_path)) \n",
    "    cifar10 = dataset\n",
    "    class_list = cifar10.get_cls_num_list()\n",
    "    class_freq = {}\n",
    "  \n",
    "    for i, x in zip(cifar10.class_to_idx, class_list):\n",
    "        class_freq[cifar10.class_to_idx[i]] = x\n",
    "\n",
    "    smallest = class_freq[5]\n",
    "\n",
    "\n",
    "    model.eval()   \n",
    "    accuracy_9 = 0\n",
    "    num_samples = smallest\n",
    "    class_to_samples = {}\n",
    "    for k in range(10):\n",
    "        \n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs) \n",
    "                preds = torch.max(outputs, 1)[1]\n",
    "                softmax = nn.Softmax(dim=1)\n",
    "                prob = softmax(outputs)\n",
    "                #image by image\n",
    "                for i, y, z, x in zip(prob, inputs, labels, preds):\n",
    "                    if z.item() == k:\n",
    "                        if k not in class_to_samples:\n",
    "                            class_to_samples[k]= [[y, z, i[k]]]\n",
    "                        else:\n",
    "                            class_to_samples[k] += [[y, z, i[k]]]\n",
    "    \n",
    "  \n",
    "    no_sampling = [5, 6, 7, 8, 9]\n",
    "    dataset_dct = {}\n",
    " \n",
    "    for i in class_to_samples:\n",
    "        class_to_samples[i] = sorted(class_to_samples[i], key=itemgetter(2))\n",
    "  \n",
    "    for i in class_to_samples:\n",
    "        if i in no_sampling:\n",
    "            dataset_dct[i] = []\n",
    "            for s in class_to_samples[i]: \n",
    "                dataset_dct[i] += [s]\n",
    "                \n",
    "        dataset_dct[i] = []\n",
    "        count = 0\n",
    "        for s in class_to_samples[i]:\n",
    "            if count == smallest:\n",
    "                break\n",
    "            count += 1\n",
    "            dataset_dct[i] += [s]\n",
    "    \n",
    "#     for i in dataset_dct:\n",
    "#         for s in dataset_dct[i]:\n",
    "#             print(len(dataset_dct[i]), s)\n",
    "   \n",
    "    lst_images = []\n",
    "    lst_labels = []\n",
    "    for i in dataset_dct:\n",
    "        for s in dataset_dct[i]:\n",
    "            lst_images.append(s[0])\n",
    "            lst_labels.append(s[1])\n",
    "   \n",
    "    dataset_sampled = dataset_most_inform(lst_images, lst_labels)\n",
    "    New_dataloader = {'train': torch.utils.data.DataLoader(dataset_sampled, batch_size=32, shuffle=True)}#, num_workers=16)}\n",
    "\n",
    "    return New_dataloader, len(dataset_sampled)\n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def MAL(model, criterion, optimizer, do_meta=False, eps=0.1,total_epochs=50, pretrained_epochs=25,inner_epochs=10, save_path='saved_weight.pth'):\n",
    "\n",
    "\n",
    "    # pretrain the model\n",
    "    train_model(model, criterion, optimizer, pretrained_epochs, dataloaders,dataset_sizes['train'], save_path='saved_weight.pth')\n",
    "    \n",
    "    \n",
    "    # load the data and sample the data\n",
    "    cifar10 = IMBALANCECIFAR10(root='./data')\n",
    "    meta_epochs = total_epochs - pretrained_epochs\n",
    "    iter_1_loader, sampled_size = sampling(model, cifar10)\n",
    "    original_datasize = dataset_sizes['train']\n",
    "    \n",
    "    # do the meta epochs  \n",
    "    for meta_epoch in range(int(original_datasize*meta_epochs/(sampled_size*inner_epochs))):\n",
    "        prev_model_params = deepcopy(model.state_dict())\n",
    "        train_model(model, criterion, optimizer, inner_epochs, iter_1_loader, sampled_size, save_path='saved_weight.pth')\n",
    "        current_model_params = deepcopy(model.state_dict())\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        for param_name in current_model_params.keys():\n",
    "            current_model_params[param_name] = (1-eps)*prev_model_params[param_name] + eps*current_model_params[param_name]\n",
    "        if do_meta:\n",
    "            model.load_state_dict(current_model_params)\n",
    "        \n",
    "        iter_1_loader, sampled_size = sampling(model, cifar10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpRAJHrtVO9o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model_mynet_ft, (3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please do the following experiments:\n",
    "1. train the model on Balanced dataset, record the performance\n",
    "2. train the model on imbalanced dataset, record the performance\n",
    "3. train the model on imbalanced dataset, with iterative learning strategy\n",
    "4. train the model on imbalanced dataset, with meta iterative learning strategy # do_mate=True\n",
    "\n",
    "set the eps from 0.1 to 0.9. step size = 0.1, Record the best.\n",
    "\n",
    "set the inner epochs from 5 to 20, step size=5.\n",
    "\n",
    "use the accuracy, precision, recall, f1 score to measure the final performance. The detailed information is here.https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "\n",
    "## Each experiments run 5 times. Record the mean and variance of each metrics.\n",
    "\n",
    "### The most important thing: Reset the model parameters before excuting the steps.\n",
    "\n",
    "Fixed hyperparameters: the transformation step is fixed.\n",
    "    Total epoch = 50, pretrained_epochs=20, lr=1e-4\n",
    "Flexible hyperparameters: \n",
    "    eps, inner_epochs, \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mynet_ft.parameters(), lr=1e-4)\n",
    "#model_mynet_ft = train_model(model_mynet_ft, criterion, optimizer, num_epochs=1, save_path='saved_own_loop.pth')\n",
    "MAL(model_mynet_ft, criterion, optimizer,do_meta=True, eps=0.9,pretrained_epochs=1,inner_epochs=1, save_path='saved_weight.pth')\n",
    "test_model(model_mynet_ft, load_path='saved_weight.pth')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_model(model_mynet_ft, load_path='saved_weight.pth')\n",
    "\n",
    "#assert acc>0.8, 'Please fune-tune your model to reach a higher accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_Image_Classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
